\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\babel@aux[2]{}
\@nameuse{bbl@beforestart}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@input{portadaTesis.aux}
\babel@aux{english}{}
\citation{parkinsonhistoria}
\citation{parkinson2002essay}
\citation{sveinbjornsdottir2016clinical}
\citation{frank2006approach}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{4}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Context}{4}{subsection.1.1}\protected@file@percent }
\citation{handsonmachinelearning}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Objective}{6}{subsection.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}Document Structure}{7}{subsection.1.3}\protected@file@percent }
\citation{deeplearningygrabaciones}
\citation{belic2019artificial}
\citation{cnntoformantmeasures}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4}State Of Art}{8}{subsection.1.4}\protected@file@percent }
\citation{handsonmachinelearning}
\@writefile{toc}{\contentsline {section}{\numberline {2}Theoretical Bases}{9}{section.2}\protected@file@percent }
\newlabel{sec:TheoreticalBases}{{2}{9}{Theoretical Bases}{section.2}{}}
\citation{handsonmachinelearning}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Types of Machine Learning Systems }{10}{subsection.2.1}\protected@file@percent }
\citation{cortex58}
\citation{cortex59}
\citation{neocognitron}
\citation{handsonmachinelearning}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}The CNN Model and VGG Architecture}{12}{subsection.2.2}\protected@file@percent }
\citation{handsonmachinelearning}
\citation{VGGinfo}
\citation{VGGimg}
\newlabel{ConvLayer}{{2.2}{13}{The CNN Model and VGG Architecture}{subsection.2.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Visual representation of the effects of convolution layers.\cite  {handsonmachinelearning}}}{13}{figure.1}\protected@file@percent }
\newlabel{MaxPoolLayer}{{2.2}{13}{The CNN Model and VGG Architecture}{figure.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Visual representation of the effects of pooling layers. \cite  {handsonmachinelearning}}}{13}{figure.2}\protected@file@percent }
\newlabel{VGGArchitecture}{{2.2}{14}{The CNN Model and VGG Architecture}{figure.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Visual representation of a VGG architecture. \cite  {VGGimg}}}{14}{figure.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}Data}{15}{section.3}\protected@file@percent }
\newlabel{sec:Data}{{3}{15}{Data}{section.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Cleaning The Data Set }{17}{subsection.3.1}\protected@file@percent }
\citation{normalization}
\citation{normalizationTechniques}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Normalizing The Data Set And Transforming Non Numerical Values }{19}{subsection.3.2}\protected@file@percent }
\newlabel{Feature02N}{{3.2}{19}{Normalizing The Data Set And Transforming Non Numerical Values}{subsection.3.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Histogram for a normalized attribute with outliers. Feature02}}{19}{figure.4}\protected@file@percent }
\newlabel{Feature24N}{{3.2}{20}{Normalizing The Data Set And Transforming Non Numerical Values}{figure.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Histogram for a normalized attribute with outliers. Feature24}}{20}{figure.5}\protected@file@percent }
\newlabel{Feature24NC}{{3.2}{20}{Normalizing The Data Set And Transforming Non Numerical Values}{figure.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Histogram for a normalized and clipped attribute. Feature24}}{20}{figure.6}\protected@file@percent }
\citation{datascienceZscore}
\citation{normalizationTechniques}
\citation{normalizeSkLearn}
\newlabel{Feature03Comparidson}{{3.2}{22}{Normalizing The Data Set And Transforming Non Numerical Values}{lstnumber.-5.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces width=50mm}}{22}{figure.7}\protected@file@percent }
\citation{dateTestsDefs}
\citation{handsonmachinelearning}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Splitting The Data Set}{23}{subsection.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Generating The Labels}{25}{subsection.3.4}\protected@file@percent }
\citation{VGG16implementation}
\@writefile{toc}{\contentsline {section}{\numberline {4}Implementation of the CNN}{26}{section.4}\protected@file@percent }
\citation{learningRates}
\citation{learningRates}
\newlabel{bigM1Squema}{{4}{27}{Implementation of the CNN}{lstnumber.-9.25}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Effects of different learning rates on the learning process of a model. \cite  {learningRates}}}{27}{figure.8}\protected@file@percent }
\citation{kerasDocs}
\citation{binnaryCrossentropy}
\citation{learningRates}
\citation{learningRates}
\newlabel{learningRates}{{4}{29}{Implementation of the CNN}{figure.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Effects of different learning rates on the learning process of a model. \cite  {learningRates}}}{29}{figure.9}\protected@file@percent }
\citation{RMSprop}
\citation{regularization}
\citation{Vanishing}
\citation{Vanishing}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}The Exploding Gradient Problem.}{32}{subsection.4.1}\protected@file@percent }
\newlabel{explodingGradientProblem}{{4.1}{32}{The Exploding Gradient Problem}{subsection.4.1}{}}
\newlabel{SigmoidAndDerivative}{{4.1}{32}{The Exploding Gradient Problem}{subsection.4.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Sigmoid function and itÂ´s derivative. \cite  {Vanishing}}}{32}{figure.10}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}The Problem With the Balance of the Data}{35}{subsection.4.2}\protected@file@percent }
\newlabel{problemUnbalanceData}{{4.2}{35}{The Problem With the Balance of the Data}{subsection.4.2}{}}
\newlabel{UnbalancedConfusionMatrix}{{4.2}{36}{The Problem With the Balance of the Data}{lstnumber.-11.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Confusion Matrix for the predictions of the model trained with unbalanced data.}}{36}{figure.11}\protected@file@percent }
\newlabel{UnbalancedTrainDataSet}{{4.2}{37}{The Problem With the Balance of the Data}{figure.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Label distribution for the unbalanced train data set.}}{37}{figure.12}\protected@file@percent }
\citation{classWeights}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.1}Implementing Weighted classes with SkLearn}{38}{subsubsection.4.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.2}Stablishing A clasification System Based on the UPDRS}{40}{subsubsection.4.2.2}\protected@file@percent }
\newlabel{UPDRSSumDistribution}{{4.2.2}{41}{Stablishing A clasification System Based on the UPDRS}{subsubsection.4.2.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Distribution for the sum of the different attributes associated to the UPDRS within the data set.}}{41}{figure.13}\protected@file@percent }
\newlabel{UPDRSSumDistributionClasses}{{4.2.2}{43}{Stablishing A clasification System Based on the UPDRS}{lstnumber.-13.23}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces Distribution for the classes created based of sum of the different attributes associated to the UPDRS within the data set.}}{43}{figure.14}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5}Conclusions}{51}{section.5}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Table with all the results for the trials that have been done over the second model of the multiclass clasification approach.}}{51}{table.1}\protected@file@percent }
\newlabel{MulticlassSecondModelResultTable}{{1}{51}{Table with all the results for the trials that have been done over the second model of the multiclass clasification approach}{table.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Possible Future Work Lines}{53}{subsection.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Social Implications}{54}{subsection.5.2}\protected@file@percent }
\bibdata{bibliografia.bib}
\bibcite{parkinsonhistoria}{1}
\bibcite{parkinson2002essay}{2}
\bibcite{sveinbjornsdottir2016clinical}{3}
\bibcite{frank2006approach}{4}
\bibcite{handsonmachinelearning}{5}
\bibcite{deeplearningygrabaciones}{6}
\bibcite{belic2019artificial}{7}
\bibcite{cnntoformantmeasures}{8}
\bibcite{cortex58}{9}
\bibcite{cortex59}{10}
\bibcite{neocognitron}{11}
\bibcite{VGGinfo}{12}
\bibcite{VGGimg}{13}
\bibcite{normalization}{14}
\bibcite{normalizationTechniques}{15}
\bibcite{datascienceZscore}{16}
\bibcite{normalizeSkLearn}{17}
\bibcite{dateTestsDefs}{18}
\bibcite{VGG16implementation}{19}
\bibcite{kerasDocs}{20}
\bibcite{binnaryCrossentropy}{21}
\bibcite{learningRates}{22}
\bibcite{RMSprop}{23}
\bibcite{regularization}{24}
\bibcite{Vanishing}{25}
\bibcite{classWeights}{26}
\bibstyle{IEEEtran}
\gdef \@abspage@last{57}
